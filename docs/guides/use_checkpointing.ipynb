{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e9134fa",
   "metadata": {},
   "source": [
    "# Use Checkpointing\n",
    "\n",
    "`flax.checkpoints` library is a simplistic, generic library to save and load model parameters, metadata and a variety of Python data. It also provides basic feature for versioning, automatic bookkeeping of past checkpoints, and async saving to reduce training wait time.\n",
    "\n",
    "In this example you will find:\n",
    "\n",
    "* Basic save/load of checkpoints.\n",
    "* More flexible and sustainable ways to load checkpoints.\n",
    "* How to save/load checkpoints when you run in multi-host scenarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ece792",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e80f8743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 8 fake devices to mimic multihost checkpointing\n",
    "import os\n",
    "os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8'\n",
    "\n",
    "from typing import Optional, Any\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import jax\n",
    "from jax import random, numpy as jnp\n",
    "from jax.experimental import maps, PartitionSpec, pjit\n",
    "from jax.experimental.gda_serialization.serialization import GlobalAsyncCheckpointManager\n",
    "\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "from flax.training import checkpoints, train_state\n",
    "from flax import struct, serialization\n",
    "import optax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d434cd",
   "metadata": {},
   "source": [
    "## Saving Checkpoints\n",
    "\n",
    "Flax checkpointing can save and load any given [PyTrees](https://jax.readthedocs.io/en/latest/pytrees.html). This includes not only typical Python and Numpy containers, but also customized classes like `dataclass`. That means you can store almost any data generated - not only your model params, but any arrays/dicts, metadata, etc.\n",
    "\n",
    "Let's create a pytree with many data structures and containers and play with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56dec3f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': TrainState(step=0, apply_fn=<bound method Module.apply of Dense(\n",
       "     # attributes\n",
       "     features = 3\n",
       "     use_bias = True\n",
       "     dtype = None\n",
       "     param_dtype = float32\n",
       "     precision = None\n",
       "     kernel_init = init\n",
       "     bias_init = zeros\n",
       " )>, params=FrozenDict({\n",
       "     kernel: DeviceArray([[ 0.26148954, -0.6129929 , -0.23358513],\n",
       "                  [ 0.11150402, -0.8755793 ,  0.9810635 ],\n",
       "                  [ 0.36360955,  0.18376349, -0.68460613],\n",
       "                  [-0.8509373 , -0.64067173, -0.48081222],\n",
       "                  [-0.6876102 , -0.33887318, -0.05798903]], dtype=float32),\n",
       "     bias: DeviceArray([0., 0., 0.], dtype=float32),\n",
       " }), tx=GradientTransformation(init=<function chain.<locals>.init_fn at 0x7fc5a31f9170>, update=<function chain.<locals>.update_fn at 0x7fc5a31f9a70>), opt_state=(EmptyState(), EmptyState())),\n",
       " 'metadata': {'dimensions': ['dense', array([5, 3])]},\n",
       " 'data': [DeviceArray([0.59902626, 0.2172144 , 2.4202902 , 0.03266738, 1.2164948 ],            dtype=float32)]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some simple model\n",
    "key1, key2 = random.split(random.PRNGKey(0))\n",
    "x1 = random.normal(key1, (5,))      # some JAX array\n",
    "model = nn.Dense(features=3)\n",
    "variables = model.init(key2, x1)\n",
    "\n",
    "# Flax's TrainState is a pytree dataclass and is supported in checkpointing.\n",
    "# Define your class with `@struct.dataclass` decorator to make it compatible.\n",
    "state = train_state.TrainState.create(\n",
    "    apply_fn=model.apply,\n",
    "    params=variables['params'],\n",
    "    tx=optax.sgd(learning_rate=0.1))\n",
    "\n",
    "# Some nested pytree with dict, array and numpy array\n",
    "metadata = {'dimensions': ['dense', np.array([5, 3])]}\n",
    "\n",
    "# Bundle them together!\n",
    "ckpt = {'model': state, 'metadata': metadata, 'data': [x1]}\n",
    "ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc59dfa",
   "metadata": {},
   "source": [
    "Now save the checkpoint. You can add annotations like step number, prefix, etc to your checkpoint.\n",
    "\n",
    "When saving the checkpoint, Flax will bookkeep existed checkpoints based on your args. For example, with `overwrite=False`, Flax will not automatically save your checkpoint if a step equal or newer is in the checkpoint directory. With `keep=2` Flax will keep a maximum of 2 checkpoints in the directory. Explore [API reference](https://flax.readthedocs.io/en/latest/api_reference/flax.training.html#module-flax.training.checkpoints) for more options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cdb35ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tmp/flax-checkpointing/checkpoint_0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_dir = 'tmp/flax-checkpointing'\n",
    "if os.path.exists(ckpt_dir):\n",
    "    shutil.rmtree(ckpt_dir)  # Remove any existed checkpoints from last notebook run\n",
    "checkpoints.save_checkpoint(ckpt_dir, ckpt, step=0, overwrite=False, keep=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b658bd1",
   "metadata": {},
   "source": [
    "## Restoring Checkpoints\n",
    "\n",
    "To restore the checkpoint, pass in the checkpoint directory. Flax will automatically select the latest checkpoint in the directory. You can also choose to specify a step number or the path of the checkpoint file.\n",
    "\n",
    "You could always restore a Pytree out of your checkpoints with `target=None`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "150b20a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': {'step': 0,\n",
       "  'params': {'kernel': array([[ 0.26148954, -0.6129929 , -0.23358513],\n",
       "          [ 0.11150402, -0.8755793 ,  0.9810635 ],\n",
       "          [ 0.36360955,  0.18376349, -0.68460613],\n",
       "          [-0.8509373 , -0.64067173, -0.48081222],\n",
       "          [-0.6876102 , -0.33887318, -0.05798903]], dtype=float32),\n",
       "   'bias': array([0., 0., 0.], dtype=float32)},\n",
       "  'opt_state': {'0': {}, '1': {}}},\n",
       " 'metadata': {'dimensions': {'0': 'dense', '1': array([5, 3])}},\n",
       " 'data': {'0': array([0.59902626, 0.2172144 , 2.4202902 , 0.03266738, 1.2164948 ],\n",
       "        dtype=float32)}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_restored = checkpoints.restore_checkpoint(ckpt_dir, target=None)\n",
    "raw_restored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987b981f",
   "metadata": {},
   "source": [
    "However, when using `target=None`, the restored `raw_restored` will be different from the original `ckpt` in a couple of ways:\n",
    "\n",
    "1. There is no TrainState now, and only some raw weights and Optax state numbers remained;\n",
    "1. `metadata.dimentions` and `data` should be arrays, but restored as dict with integers as keys;\n",
    "1. `data[0]` used to be a `jnp.array` but now is a `numpy.array`. \n",
    "\n",
    "While (3) would not affect future work because JAX will automatically convert Numpy arrays to JAX arrays once computation starts, (1) and (2) may lead to confusions.\n",
    "\n",
    "In order to solve this, you should pass an example `target` to let Flax know exactly what structure it should restore to. `target` should introduce any custom dataclasses explicitly, and have the same structure as the saved checkpoint.\n",
    "\n",
    "It's often recommended to refactor out the process of initializing a checkpoint's structure (e.g., a TrainState), so that saving/loading is easier and less error-prone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58f42513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': TrainState(step=0, apply_fn=<bound method Module.apply of Dense(\n",
       "     # attributes\n",
       "     features = 3\n",
       "     use_bias = True\n",
       "     dtype = None\n",
       "     param_dtype = float32\n",
       "     precision = None\n",
       "     kernel_init = init\n",
       "     bias_init = zeros\n",
       " )>, params={'kernel': array([[ 0.26148954, -0.6129929 , -0.23358513],\n",
       "        [ 0.11150402, -0.8755793 ,  0.9810635 ],\n",
       "        [ 0.36360955,  0.18376349, -0.68460613],\n",
       "        [-0.8509373 , -0.64067173, -0.48081222],\n",
       "        [-0.6876102 , -0.33887318, -0.05798903]], dtype=float32), 'bias': array([0., 0., 0.], dtype=float32)}, tx=GradientTransformation(init=<function chain.<locals>.init_fn at 0x7fc5a342d200>, update=<function chain.<locals>.update_fn at 0x7fc5a342d290>), opt_state=(EmptyState(), EmptyState())),\n",
       " 'metadata': {'dimensions': {'0': 'dense', '1': array([5, 3])}},\n",
       " 'data': [array([0.59902626, 0.2172144 , 2.4202902 , 0.03266738, 1.2164948 ],\n",
       "        dtype=float32)]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_state = train_state.TrainState.create(\n",
    "    apply_fn=model.apply,\n",
    "    params=np.zeros_like(variables['params']),  # values of the tree leaf doesn't matter\n",
    "    tx=optax.sgd(learning_rate=0.0),\n",
    ")\n",
    "target = {'model': empty_state, 'metadata': None, 'data': [jnp.zeros_like(x1)]}\n",
    "state_restored = checkpoints.restore_checkpoint(ckpt_dir, target=target, step=0)\n",
    "state_restored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136a300a",
   "metadata": {},
   "source": [
    "### Back/Forward Dataclass Compatibility\n",
    "\n",
    "The flexibility of using dataclasses means that changes in dataclass fields could break your existed checkpoints. For example, if you decide to add a field `batch_stats` to your `TrainState`, old checkpoints without this field could not be successfully restored. Same goes for removing a field in your dataclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be65d4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValueError when target state has an unmentioned field:\n",
      "Missing field batch_stats in state dict while restoring an instance of CustomTrainState\n",
      "\n",
      "ValueError when target state misses a recorded field:\n",
      "Unknown field(s) \"batch_stats\" in state dict while restoring an instance of TrainState\n"
     ]
    }
   ],
   "source": [
    "class CustomTrainState(train_state.TrainState):\n",
    "    batch_stats: Any = None\n",
    "\n",
    "custom_state = CustomTrainState.create(\n",
    "    apply_fn=state.apply_fn,\n",
    "    params=state.params,\n",
    "    tx=state.tx,\n",
    "    batch_stats=np.arange(10),\n",
    ")\n",
    "\n",
    "# Use custom state to read old TrainState checkpoint\n",
    "custom_target = {'model': custom_state, 'metadata': None, 'data': [jnp.zeros_like(x1)]}\n",
    "try:\n",
    "    checkpoints.restore_checkpoint(ckpt_dir, target=custom_target, step=0)\n",
    "except ValueError as e:\n",
    "    print('ValueError when target state has an unmentioned field:')\n",
    "    print(e)\n",
    "    print('')\n",
    "\n",
    "\n",
    "# Use old TrainState to read the custom state checkpoint\n",
    "custom_ckpt = {'model': custom_state, 'metadata': metadata, 'data': [x1]}\n",
    "checkpoints.save_checkpoint(ckpt_dir, custom_ckpt, step=1, overwrite=True, keep=2)\n",
    "try:\n",
    "    checkpoints.restore_checkpoint(ckpt_dir, target=target, step=1)\n",
    "except ValueError as e:\n",
    "    print('ValueError when target state misses a recorded field:')\n",
    "    print(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eccbc2c",
   "metadata": {},
   "source": [
    "It is recommended to keep your checkpoints up to date with your pytree dataclass definitions. But if you must restore checkpoints and dataclasses with incompatible fields, you could manually add/remove corresponding fields before passing in the correct target structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29fd1e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': CustomTrainState(step=0, apply_fn=<bound method Module.apply of Dense(\n",
       "     # attributes\n",
       "     features = 3\n",
       "     use_bias = True\n",
       "     dtype = None\n",
       "     param_dtype = float32\n",
       "     precision = None\n",
       "     kernel_init = init\n",
       "     bias_init = zeros\n",
       " )>, params=FrozenDict({\n",
       "     kernel: array([[ 0.26148954, -0.6129929 , -0.23358513],\n",
       "            [ 0.11150402, -0.8755793 ,  0.9810635 ],\n",
       "            [ 0.36360955,  0.18376349, -0.68460613],\n",
       "            [-0.8509373 , -0.64067173, -0.48081222],\n",
       "            [-0.6876102 , -0.33887318, -0.05798903]], dtype=float32),\n",
       "     bias: array([0., 0., 0.], dtype=float32),\n",
       " }), tx=GradientTransformation(init=<function chain.<locals>.init_fn at 0x7fc5a31f9170>, update=<function chain.<locals>.update_fn at 0x7fc5a31f9a70>), opt_state=(EmptyState(), EmptyState()), batch_stats=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])),\n",
       " 'metadata': {'dimensions': {'0': 'dense', '1': array([5, 3])}},\n",
       " 'data': [array([0.59902626, 0.2172144 , 2.4202902 , 0.03266738, 1.2164948 ],\n",
       "        dtype=float32)]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass no target to get a raw state dictionary first\n",
    "raw_state_dict = checkpoints.restore_checkpoint(ckpt_dir, target=None, step=0)\n",
    "# Add/remove fields as needed.\n",
    "raw_state_dict['model']['batch_stats'] = np.arange(10)\n",
    "# Restore the classes with correct target now\n",
    "serialization.from_state_dict(custom_target, raw_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b39501",
   "metadata": {},
   "source": [
    "## Asynchronized Checkpointing\n",
    "\n",
    "Checkpointing is I/O heavy and if you have large amount of data to save, it may be worthwhile to put it into a background thread and continue with your training meanwhile. You could do that by creating an `async_manager` and let it track your save thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85be68a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:The previous async save_checkpoint has not finished yet. Waiting for it to complete before the next save.\n"
     ]
    }
   ],
   "source": [
    "am = checkpoints.AsyncManager()\n",
    "checkpoints.save_checkpoint(ckpt_dir, ckpt, step=2, overwrite=True, keep=3, async_manager=am)\n",
    "\n",
    "# ... Continue with your work...\n",
    "# ... Until a time, when you want to exit your process or wait until the save completes:\n",
    "am.wait_previous_save()  # Block until save completes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e93db6",
   "metadata": {},
   "source": [
    "## Multihost/multiprocess Checkpointing\n",
    "\n",
    "JAX provided a few ways to scale up your code on multiple hosts at the same time. This usually happens when the number of devices (CPU/GPU/TPU) is so large that different devices are managed by different hosts (CPU). For an overview of JAX in multi-process settings, read [here](https://jax.readthedocs.io/en/latest/multi_process.html).\n",
    "\n",
    "In SPMD paradigm with `jax.pjit` ([tutorial](https://jax.readthedocs.io/en/latest/jax-101/08-pjit.html)), a large, multiprocess array could have its data shards on different devices. These data arrays needs a special JAX API `GlobalAsyncCheckpointManager` to save and restore. This API lets each host to dump its data shards to a single shared storage, e.g. a Google Cloud bucket. \n",
    "\n",
    "Flax provide easy interface for users to pass in a `GlobalAsyncCheckpointManager` and store pytrees with multi-process arrays in the same fashion as single-process pytrees. Just use `checkpoints.save_checkpoint_multiprocess()` with the same arguments.\n",
    "\n",
    "Unfortunately Python notebooks are single-host only and cannot activate the multi-host mode. Use the following code as a sample to run your multi-host checkpointing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d199c8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In reality you should set this with multiple num_processes\n",
    "# See https://jax.readthedocs.io/en/latest/multi_process.html#initializing-the-cluster\n",
    "jax.distributed.initialize(\"localhost:8889\", num_processes=1, process_id=0)\n",
    "\n",
    "# Create a multiprocess array\n",
    "jax.config.update('jax_array', True)\n",
    "mesh_shape = (4, 2)\n",
    "devices = np.asarray(jax.devices()).reshape(*mesh_shape)\n",
    "mesh = maps.Mesh(devices, ('x', 'y'))\n",
    "f = pjit.pjit(\n",
    "  lambda x: x,\n",
    "  in_axis_resources=None,\n",
    "  out_axis_resources=PartitionSpec('x', 'y'))\n",
    "with maps.Mesh(mesh.devices, mesh.axis_names):\n",
    "    mp_array = f(np.arange(8 * 2).reshape(8, 2))\n",
    "\n",
    "# Make it a pytree as normal\n",
    "mp_ckpt = {'model': mp_array}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57231d6",
   "metadata": {},
   "source": [
    "Sample call to save the checkpoint. Note that all the arguments are the same as `save_checkpoint`, except for an additional `gda_manager` argument. \n",
    "\n",
    "If your checkpoint is too large, you could add `timeout_secs` to the manager and gives it more time to finish writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d10039b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tmp/flax-checkpointing/checkpoint_3'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gacm = GlobalAsyncCheckpointManager(timeout_secs=50)\n",
    "checkpoints.save_checkpoint_multiprocess(ckpt_dir, mp_ckpt, step=3, overwrite=True, \n",
    "                                         keep=4, gda_manager=gacm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6407d018",
   "metadata": {},
   "source": [
    "Sample code to restore the checkpoint. \n",
    "\n",
    "Note that you need to pass a target with valid multiprocess arrays at the correct structual location. Flax only uses the target arrays' meshes and mesh axes to restore the checkpoint, so the array itself need not to be as large as your checkpoint's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9f9724c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': array([[ 0,  1],\n",
       "        [ 2,  3],\n",
       "        [ 4,  5],\n",
       "        [ 6,  7],\n",
       "        [ 8,  9],\n",
       "        [10, 11],\n",
       "        [12, 13],\n",
       "        [14, 15]], dtype=int32)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with maps.Mesh(mesh.devices, mesh.axis_names):\n",
    "    mp_smaller_array = f(np.zeros(8).reshape(4, 2))\n",
    "\n",
    "mp_target = {'model': mp_smaller_array}\n",
    "mp_restored = checkpoints.restore_checkpoint(ckpt_dir, target=mp_target, \n",
    "                                             step=3, gda_manager=gacm)\n",
    "mp_restored"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
