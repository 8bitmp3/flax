{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why NNX?\n",
    "\n",
    "Four years ago we developed the Flax \"Linen\" API to support modeling research on JAX, with a focus on scaling scaling and performance.  We've learned a lot from our users over these years.\n",
    "\n",
    "We introduced some ideas that have proven to be good:\n",
    " - Organizing variables into [collections](https://flax.readthedocs.io/en/latest/glossary.html#term-Variable-collections) or types to support JAX transforms and segregation of different data types in training loops.\n",
    " - Automatic and efficient [PRNG management](https://flax.readthedocs.io/en/latest/glossary.html#term-RNG-sequences) (with support for splitting/broadcast control across map transforms)\n",
    " - [Variable Metadata](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/_autosummary/flax.linen.with_partitioning.html#flax.linen.with_partitioning) for SPMD annotations, optimizer metadata, and other uses.\n",
    "\n",
    "One choice we made was to use functional \"define by call\" semantics for NN programming via the lazy (ie just in time) initialization of parameters.  This made for  concise (`compact`) implementation code and allowed for a single specification when transforming a layer.  It also aligned our API to be closer to Haiku.  However that lazy-init meant that the semantics of variables in Flax were non-pythonic and often surprising.  It also led to implementation complexity and obscured the core ideas of transformations on neural nets.\n",
    "\n",
    "NNX is an attempt to keep the features that made Linen great while introducing some new principles:\n",
    "\n",
    "- Regular Python semantics for Modules, including (within JIT boundaries) support for mutability and shared references.\n",
    "- A simple API to interact directly with the JAX, this includes the ability to easily implement custom lifted Modules and other purely functional tricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NNX is Pythonic\n",
    "The main feature of NNX Module is that it adheres to Python semantics. This means that:\n",
    "\n",
    "* fields are mutable so you can perform inplace updates\n",
    "* Module references can be shared between multiple Modules\n",
    "* Module construction implies parameter initialization\n",
    "* Module methods can be called directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A Google TPU may be present on this machine, but either a TPU-enabled jaxlib or libtpu is not installed. Falling back to cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model = CounterLinear(\n",
      "  linear=Linear(\n",
      "    in_features=4,\n",
      "    out_features=4,\n",
      "    use_bias=True,\n",
      "    dtype=None,\n",
      "    param_dtype=<class 'jax.numpy.float32'>,\n",
      "    precision=None,\n",
      "    kernel_init=<function variance_scaling.<locals>.init at 0x7f5d3c57baf0>,\n",
      "    bias_init=<function zeros at 0x7f5ddf0e4ca0>,\n",
      "    dot_general=<function dot_general at 0x7f5ddf79d4c0>\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from flax.experimental import nnx\n",
    "import jax\n",
    "from jax import random, numpy as jnp\n",
    "\n",
    "class Count(nnx.Variable): pass\n",
    "\n",
    "class CounterLinear(nnx.Module):\n",
    "  def __init__(self, din, dout, *, rngs): # explicit RNG threading\n",
    "    self.linear = nnx.Linear(din, dout, rngs=rngs)\n",
    "    self.count = Count(jnp.zeros((), jnp.int32)) # typed Variable collections\n",
    "\n",
    "  def __call__(self, x):\n",
    "    self.count += 1  # inplace stateful updates\n",
    "    return self.linear(x)\n",
    "\n",
    "\n",
    "model = CounterLinear(4, 4, rngs=nnx.Rngs(0))  # no special `init` method\n",
    "y = model(jnp.ones((2, 4)))  # call methods directly\n",
    "\n",
    "print(f'{model = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because NNX Modules contain their own state, they are very easily to inspect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.count = Array(1, dtype=int32)\n",
      "model.linear.kernel = Array([[ 0.4541089 , -0.5264876 , -0.36505195, -0.57566494],\n",
      "       [ 0.38802508,  0.5655534 ,  0.4870657 ,  0.2267774 ],\n",
      "       [-0.9015767 ,  0.24465278, -0.5844707 ,  0.18421966],\n",
      "       [-0.06992685, -0.64693886,  0.20232596,  1.1200062 ]],      dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(f'{model.count = }')\n",
    "print(f'{model.linear.kernel = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intuitive Surgery\n",
    "\n",
    "In NNX surgery can be done at the Module level by simply updating / replacing existing fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained():\n",
    "  return nnx.Linear(4, 4, rngs=nnx.Rngs(42))  # pretend this is pretrained\n",
    "\n",
    "model.linear = load_pretrained()  # you can replace modules\n",
    "\n",
    "y = model(jnp.ones((2, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benefit of this is not only that its easier than messing with dictionary structures, but can even replace a field with a completely different Module type, or even change the architecture (e.g. share two Modules that were not shared before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "rngs = nnx.Rngs(0)\n",
    "model = nnx.Sequence(\n",
    "  [\n",
    "    nnx.Conv(1, 16, [3, 3], padding='SAME', rngs=rngs),\n",
    "    partial(nnx.max_pool, window_shape=(2, 2), strides=(2, 2)),\n",
    "    nnx.Conv(16, 32, [3, 3], padding='SAME', rngs=rngs),\n",
    "    partial(nnx.max_pool, window_shape=(2, 2), strides=(2, 2)),\n",
    "    lambda x: x.reshape((x.shape[0], -1)),  # flatten\n",
    "    nnx.Linear(32 * 7 * 7, 10, rngs=rngs),\n",
    "  ]\n",
    ")\n",
    "\n",
    "y = model(jnp.ones((2, 28, 28, 1)))\n",
    "\n",
    "for i, layer in enumerate(model):\n",
    "  if isinstance(layer, nnx.Conv):\n",
    "    model[i] = nnx.Linear(layer.in_features, layer.out_features, rngs=rngs)\n",
    "\n",
    "y = model(jnp.ones((2, 28, 28, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that here we are replacing `Conv` with `Linear` as a silly example, but in reality you would do things like replacing a layer with its quantized version, or changing a layer with an optimized version, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interacting with JAX is easy\n",
    "\n",
    "While NNX Modules inherently follow reference semantics, they can be easily converted into a pure functional representation that can be used with JAX transformations. NNX has two very simple APIs to interact with JAX: `split` and `merge`.\n",
    "\n",
    "The `Module.split` method allows you to convert into a `State` dict-like object that contains the dynamic state of the Module, and a `ModuleDef` object that contains the static structure of the Module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state = State({\n",
      "  'count': Array(0, dtype=int32),\n",
      "  'linear/bias': Array([0., 0., 0., 0.], dtype=float32),\n",
      "  'linear/kernel': Array([[ 0.4541089 , -0.5264876 , -0.36505195, -0.57566494],\n",
      "         [ 0.38802508,  0.5655534 ,  0.4870657 ,  0.2267774 ],\n",
      "         [-0.9015767 ,  0.24465278, -0.5844707 ,  0.18421966],\n",
      "         [-0.06992685, -0.64693886,  0.20232596,  1.1200062 ]],      dtype=float32)\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "model = CounterLinear(4, 4, rngs=nnx.Rngs(0))\n",
    "\n",
    "state, static = model.split()\n",
    "\n",
    "print(f'{state = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ModuleDef.merge` method allows you to take a `ModuleDef` and one or more `State` objects and merge them back into a `Module` object. \n",
    "\n",
    "Using `split` and `merge` in conjunction allows you to carry your Module in and out of any JAX transformation. Here is a simple jitted `forward` function as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y.shape = (2, 4)\n",
      "state[\"count\"] = Array(1, dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "@jax.jit\n",
    "def forward(state: nnx.State, x: jax.Array):\n",
    "  model = static.merge(state)\n",
    "  y = model(x)\n",
    "  state, _ = model.split()\n",
    "  return y, state\n",
    "\n",
    "x = jnp.ones((2, 4))\n",
    "y, state = forward(state, x)\n",
    "\n",
    "print(f'{y.shape = }')\n",
    "print(f'{state[\"count\"] = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom lifted Modules\n",
    "\n",
    "By using the same mechanism inside Module methods you can implement lifted Modules, that is, Modules that use a JAX transformation to have a distinct behavior. One of Linen's current pain points is that it is not easy to interact with JAX transformations that are not currently supported by the framework. NNX makes this so easy that its realistic to implement custom lifted Modules for specific use cases.\n",
    "\n",
    "As an example here we will create a `LinearEnsemble` Module that uses `jax.vmap` both during `__init__` and `__call__` to vectorize the computation over multiple `CounterLinear` models (defined above). The example is a little bit longer, but notice how each method conceptually very simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y.shape = (8, 4)\n",
      "ensemble.models.count = Array(1, dtype=int32)\n",
      "state = State({\n",
      "  'models/count': (),\n",
      "  'models/linear/bias': (8, 4),\n",
      "  'models/linear/kernel': (8, 4, 4)\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "class LinearEnsemble(nnx.Module):\n",
    "  def __init__(self, din, dout, *, num_models, rngs: nnx.Rngs):\n",
    "    # get raw rng seeds\n",
    "    keys = rngs.fork(num_models) # split all keys into `num_models`\n",
    "\n",
    "    # define pure init fn and vmap\n",
    "    def vmap_init(keys):\n",
    "      return CounterLinear(din, dout, rngs=nnx.Rngs(keys)).split(\n",
    "        nnx.Param, Count\n",
    "      )\n",
    "\n",
    "    params, counts, static = jax.vmap(\n",
    "      vmap_init, in_axes=(0,), out_axes=(0, None, None)\n",
    "    )(keys)\n",
    "    # update wrapped submodule reference\n",
    "    self.models = static.merge(params, counts)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    # get module values, define pure fn\n",
    "    params, counts, static = self.models.split(nnx.Param, Count)\n",
    "\n",
    "    def vmap_apply(x, params, counts, static):\n",
    "      model = static.merge(params, counts)\n",
    "      y = model(x)\n",
    "      params, counts, static = model.split(nnx.Param, Count)\n",
    "      return y, params, counts, static\n",
    "\n",
    "    # vmap and call\n",
    "    y, params, counts, static = jax.vmap(\n",
    "      vmap_apply, in_axes=(None, 0, None, None), out_axes=(0, 0, None, None)\n",
    "    )(x, params, counts, static)\n",
    "    # update wrapped module\n",
    "    self.models.update(params, counts, static) # use `update` to integrate the new state\n",
    "    return y\n",
    "\n",
    "x = jnp.ones((4,))\n",
    "ensemble = LinearEnsemble(4, 4, num_models=8, rngs=nnx.Rngs(0))\n",
    "\n",
    "# forward pass\n",
    "y = ensemble(x)\n",
    "\n",
    "print(f'{y.shape = }')\n",
    "print(f'{ensemble.models.count = }')\n",
    "print(f'state = {jax.tree_map(jnp.shape, ensemble.get_state())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Modules are not Pytrees?\n",
    "\n",
    "Finally one of the most common questions we get is why NNX Modules are not Pytrees? Given the existance of Pytree-based NN frameworks like Equinox, Treex, [PytreeClass](https://github.com/ASEM000/PyTreeClass), it is a fair question.\n",
    "\n",
    "The answer is that Pytrees assume value semantics (referencial transparency) while Modules assume reference semantics, and therefore its not a good idea for Modules to be Pytrees. As an example, lets take a look at what would happen if we allowed this very simple program to be valid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def f(m1: nnx.Module, m2: nnx.Module):\n",
    "  return m1, m2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are just creating a jitted function `f` that takes in two Modules `(m1, m2)` and returns them as is. What could go wrong?\n",
    "\n",
    "There are two main problems with this:\n",
    "* Shared references are not maintained, that is, if `m1.shared is m2.shared` outside `f`, this will NOT be true both inside `f`, and at the output of `f`.\n",
    "* Even if you accept this fact and added code to compensate for this, `f` would now behave differently depending on whether its being `jit`ted or not, this is an undisired asymmetry and `jit` would no longer be a no-op."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
