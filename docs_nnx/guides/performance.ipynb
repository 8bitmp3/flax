{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance considerations\n",
    "\n",
    "Currently, Flax [`nnx.jit`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/transforms.html#flax.nnx.jit) traverses the object graph in pure Python, which is slow and adds overhead. This is why in order to solve this the Flax team will be developing a Rust extension called `flaxlib` to speed up some of the traversal logic in [`graph.py`](https://github.com/google/flax/blob/main/flax/nnx/graph.py). This will be similar to how the JAX team resolved a similar issue by introducing [`jaxlib`](https://jax.readthedocs.io/en/latest/installation.html#installation) for standard [JAX pytrees](https://jax.readthedocs.io/en/latest/key-concepts.html#pytrees) (refer to the first steps in [Flax PR #4196](https://github.com/google/flax/pull/4196)).\n",
    "\n",
    "However, there are two things to consider:\n",
    "\n",
    "* The overhead is only relevant for small models (refer to [Asynchronous dispatch](#asynchronous-dispatch).\n",
    "* You can remove the overhead by using [`jax.jit`](https://jax.readthedocs.io/en/latest/_autosummary/jax.jit.html) + [`flax.nnx.split`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/graph.html#flax.nnx.split) / [`flax.nnx.merge`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/graph.html#flax.nnx.merge) to stage out the traversal logic (Refer to [Lowering the Python overhead](#lowering-the-python-overhead).\n",
    "\n",
    "\n",
    "## Asynchronous dispatch\n",
    "\n",
    "In [benchmarks/nnx_simple_training.py](https://github.com/google/flax/blob/main/benchmarks/nnx_simple_training.py) we are increasing the layer width (features per layer) and measuring the total training time for the same model trained both with [`nnx.jit`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/transforms.html#flax.nnx.jit) and [`jax.jit`](https://jax.readthedocs.io/en/latest/_autosummary/jax.jit.html).\n",
    "\n",
    "As demonstrated in the graph below, after a certain model size the time spent in the traversal is completely absorbed by async dispatch. This happens when Python is able to finish the current for loop step, and reach the next `train_step` and JAX is still not done with the previous `train_step`. \n",
    "\n",
    "![performance-graph](images/performance-graph.png)\n",
    "\n",
    "This means that you only need to worry about the `nnx.jit` overhead for small models. If you are working with a small model, check out the next section to see how you can remove the overhead.\n",
    "\n",
    "## Lowering the Python overhead\n",
    "\n",
    "To remove the Python overhead, you can use regular `jax.jit` in combination with `nnx.split` and `nnx.merge` to stage out the traversal logic.\n",
    "\n",
    "To learn how to do this, let’s first create the following simple `Model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import nnx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "class Model(nnx.Module):\n",
    "  def __init__(self, din, dmid, dout, rngs: nnx.Rngs):\n",
    "    self.linear = nnx.Linear(din, dmid, rngs=rngs)\n",
    "    self.bn = nnx.BatchNorm(dmid, rngs=rngs)\n",
    "    self.dropout = nnx.Dropout(0.2, rngs=rngs)\n",
    "    self.linear_out = nnx.Linear(dmid, dout, rngs=rngs)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    x = nnx.relu(self.dropout(self.bn(self.linear(x))))\n",
    "    return self.linear_out(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let’s create a `train_step()` function that uses [`nnx.jit`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/transforms.html#flax.nnx.jit), taking in the `model`, `optimizer`, and `metrics`, all of which are Flax NNX objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(2, 64, 3, rngs=nnx.Rngs(0))  # eager initialization\n",
    "optimizer = nnx.Optimizer(model, optax.adam(1e-3))  # reference sharing\n",
    "metrics = nnx.MultiMetric(\n",
    "  loss=nnx.metrics.Average('loss'),\n",
    ")\n",
    "\n",
    "@nnx.jit  # <== currently slow\n",
    "def train_step(model, optimizer, metrics, x, y):\n",
    "  def loss_fn(model):\n",
    "    y_pred = model(x)  # call methods directly\n",
    "    return ((y_pred - y) ** 2).mean()\n",
    "\n",
    "  loss, grads = nnx.value_and_grad(loss_fn)(model)\n",
    "  optimizer.update(grads)  # in-place updates\n",
    "  metrics.update(loss=loss)\n",
    "\n",
    "  return loss\n",
    "  \n",
    "for _ in range(10):\n",
    "  x, y = jnp.ones((32, 2)), jnp.zeros((32, 3))\n",
    "  loss = train_step(model, optimizer, metrics, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed this up, before starting the training loop we can use [`nnx.split`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/graph.html#flax.nnx.split) over all the Flax NNX objects that are inputs to `train_step()` to create `graphdef` and `state` pytrees that are faster to traverse.\n",
    "\n",
    "Next, we change `train_step()` to accept `graphdef` and `state`, and use [`nnx.merge`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/graph.html#flax.nnx.merge) and [`nnx.split`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/graph.html#flax.nnx.split) at the beginning and the end of `train_step()` to switch back and forth between the objects and their pytree representations. And even though `nnx.split` and `nnx.merge` are slow, it doesn't matter because they will run only once during tracing.\n",
    "\n",
    "With this in place, we can change the `train_step()` function to use `jax.jit` instead of `nnx.jit`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(2, 64, 3, rngs=nnx.Rngs(0))  # eager initialization\n",
    "optimizer = nnx.Optimizer(model, optax.adamw(1e-3))  # reference sharing\n",
    "metrics = nnx.MultiMetric(\n",
    "  loss=nnx.metrics.Average('loss'),\n",
    ")\n",
    "# split before training loop\n",
    "graphdef, state = nnx.split((model, optimizer, metrics))\n",
    "\n",
    "@jax.jit  # regular JAX\n",
    "def train_step(graphdef, state, x, y):\n",
    "  # merge at the beginning of the function\n",
    "  model, optimizer, metrics = nnx.merge(graphdef, state)\n",
    "\n",
    "  def loss_fn(model):\n",
    "    y_pred = model(x)  # call methods directly\n",
    "    return ((y_pred - y) ** 2).mean()\n",
    "\n",
    "  loss, grads = nnx.value_and_grad(loss_fn)(model)\n",
    "  optimizer.update(grads)\n",
    "  metrics.update(loss=loss)\n",
    "\n",
    "  # split at the end of the function\n",
    "  _, state = nnx.split((model, optimizer, metrics))\n",
    "\n",
    "  # return new state\n",
    "  return state, loss\n",
    "\n",
    "for _ in range(10):\n",
    "  x, y = jnp.ones((32, 2)), jnp.zeros((32, 3))\n",
    "  state, loss = train_step(graphdef, state, x, y)\n",
    "\n",
    "# update objects after training\n",
    "nnx.update((model, optimizer, metrics), state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we only do this for `jit`. You can still use other [Flax transforms](https://flax.readthedocs.io/en/latest/guides/transforms.html#transformations) like [`nnx.value_and_grad`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/transforms.html#flax.nnx.value_and_grad) shown in the above example since their overhead is already absorbed by the outer `jit`.\n",
    "\n",
    "And after the training loop is done (or whenever it is needed), we can use Flax [`nnx.update`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/graph.html#flax.nnx.update) to update Flax NNX objects like `model`, `optimizer`, and `metrics` to a new `state`."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
